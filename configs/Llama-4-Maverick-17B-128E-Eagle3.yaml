base: ./common.yaml

model:
  target:
    pretrained_model_name_or_path: meta-llama/Llama-4-Scout-17B-16E-Instruct
  draft:
    type: LlamaForCausalLMEagle3
    attention_bias: false
    attention_dropout: 0.0
    bos_token_id: null
    draft_vocab_size: 202048
    eagle_config:
      eagle_aux_hidden_state_layer_ids: [1, 23, 44]
      use_aux_hidden_state: true
      use_input_layernorm_in_first_layer: true
      use_last_layernorm: true
      use_mtp_layernorm: false
    eos_token_id: null
    head_dim: 128
    hidden_act: "silu"
    hidden_size: 5120
    initializer_range: 0.02
    intermediate_size: 32768
    max_position_embeddings: 1048576
    mlp_bias: false
    model_type: "llama"
    num_attention_heads: 40
    num_hidden_layers: 1
    num_key_value_heads: 8
    pretraining_tp: 1
    rms_norm_eps: 1e-05
    rope_scaling:
      factor: 8.0
      high_freq_factor: 4.0
      low_freq_factor: 1.0
      original_max_position_embeddings: 8192
      rope_type: "llama3"
    rope_theta: 500000.0
    tie_word_embeddings: false
    torch_dtype: "bfloat16"
    use_cache: true
    vocab_size: 202048
