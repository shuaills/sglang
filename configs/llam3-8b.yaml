base: ./common.yaml


output_dir: /sgl-workspace/llama3_draft

data:
  train:
    path: /sgl-workspace/BaldEagle/outdir0/1/
    batch_size: 1

model:
  target:
    pretrained_model_name_or_path: /root/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/
  draft:
    type: LlamaForCausalLMEagle3
    attention_bias: false
    attention_dropout: 0.0
    bos_token_id: null
    draft_vocab_size: 202048
    eagle_config:
      eagle_aux_hidden_state_layer_ids: [1, 23, 44]
      use_aux_hidden_state: true
      use_input_layernorm_in_first_layer: true
      use_last_layernorm: true
      use_mtp_layernorm: false
    eos_token_id: null
    head_dim: 128
    hidden_act: "silu"
    hidden_size: 4096
    initializer_range: 0.02
    intermediate_size: 14336
    max_position_embeddings: 131072
    mlp_bias: false
    model_type: "llama"
    num_attention_heads: 32
    num_hidden_layers: 1
    num_key_value_heads: 8
    pretraining_tp: 1
    rms_norm_eps: 1e-05
    rope_scaling:
      factor: 8.0
      high_freq_factor: 4.0
      low_freq_factor: 1.0
      original_max_position_embeddings: 8192
      rope_type: "llama3"
    rope_theta: 500000.0
    tie_word_embeddings: false
    torch_dtype: "bfloat16"
    use_cache: true
    vocab_size: 128256
