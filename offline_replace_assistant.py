"""Replace assistant responses in a JSONL dataset using SGLang Engine.

Usage:
    python offline_replace_assistant.py --model-path MODEL \
        --input data.jsonl --output updated.jsonl

The script reads a multi-turn chat dataset in JSONL format and replaces each
assistant message with text generated by the specified model.  Data parallelism
can be enabled via ``--dp-size`` to speed up processing of large datasets.
"""

import argparse
import asyncio
import dataclasses
import json
import logging
import time
from typing import Any, Dict, List

import sglang as sgl
from sglang.srt.conversation import chat_templates, get_conv_template_by_model_path
from sglang.srt.server_args import ServerArgs
from tqdm import tqdm
from transformers import AutoTokenizer

# Suppress SGLang INFO logs to reduce noise
logging.getLogger("sglang").setLevel(logging.WARNING)
logging.getLogger().setLevel(logging.WARNING)


class ProgressTracker:
    def __init__(self, total: int):
        self.total = total
        self.completed = 0
        self.start_time = time.time()
        
    def update(self, count: int = 1):
        self.completed += count
        
    def get_stats(self):
        elapsed = time.time() - self.start_time
        rate = self.completed / elapsed if elapsed > 0 else 0
        eta = (self.total - self.completed) / rate if rate > 0 else 0
        return {
            'completed': self.completed,
            'total': self.total,
            'rate': rate,
            'eta': eta,
            'elapsed': elapsed
        }


class InferenceEngine:
    def __init__(self, **kwargs: Any) -> None:
        self.engine = sgl.Engine(**kwargs)

    async def generate(self, prompt: str, sampling_params: Dict[str, Any]) -> str:
        result = await self.engine.async_generate(prompt, sampling_params)
        return result["text"]

    def shutdown(self) -> None:
        self.engine.shutdown()


def build_conversation_prompt(
    messages: List[Dict[str, Any]], 
    tokenizer, 
    server_args: ServerArgs
) -> str:
    """
    Build conversation prompt using HF tokenizer first, fallback to SGLang templates.
    """
    # Convert message format to OpenAI standard
    standardized_messages = []
    
    for msg in messages:
        role = msg.get("from") or msg.get("role")
        content = msg.get("value") or msg.get("content")
        
        # Standardize role names
        if role in {"human", "user"}:
            role = "user"
        elif role in {"gpt", "assistant"}:
            role = "assistant"
        elif role == "system":
            role = "system"
        else:
            continue  # Skip unknown roles
            
        if content is not None:
            standardized_messages.append({"role": role, "content": str(content)})
    
    # Method 1: Try using HF tokenizer's native chat template (recommended)
    try:
        if hasattr(tokenizer, 'chat_template') and tokenizer.chat_template is not None:
            prompt = tokenizer.apply_chat_template(
                standardized_messages,
                tokenize=False,
                add_generation_prompt=True
            )
            return prompt
    except Exception as e:
        print(f"⚠ HF chat template failed: {e}, falling back to SGLang template")
    
    # Method 2: Fallback to SGLang's template system
    chat_template_name = server_args.chat_template
    if chat_template_name is None:
        chat_template_name = get_conv_template_by_model_path(server_args.model_path)
        if chat_template_name is None:
            # Last resort: use 'llama3' as default
            chat_template_name = "llama3"
            
    print(f"⚠ Using SGLang template: {chat_template_name}")
    conv = chat_templates[chat_template_name].copy()
    
    # Ensure conv.roles has at least 2 elements
    if len(conv.roles) < 2:
        raise ValueError(f"Invalid conversation template {chat_template_name}: roles must have at least 2 elements")
    
    for msg in standardized_messages:
        if msg["role"] == "user":
            conv.append_message(conv.roles[0], msg["content"])
        elif msg["role"] == "assistant":
            conv.append_message(conv.roles[1], msg["content"])
        elif msg["role"] == "system":
            conv.system_message = msg["content"]
    
    # Add generation prompt for assistant
    conv.append_message(conv.roles[1], "")
    
    return conv.get_prompt()


async def _process_conversation(
    data: Dict[str, Any],
    engine: InferenceEngine,
    server_args: ServerArgs,
    sampling_params: Dict[str, Any],
    tokenizer,
    pbar: tqdm,
) -> Dict[str, Any]:
    """Generate assistant replies for a single conversation."""
    try:
        messages: List[Dict[str, Any]] = data.get("conversations") or data.get("messages")
        if messages is None or len(messages) == 0:
            return data

        # Build conversation up to the last user message
        # We'll regenerate all assistant responses
        conversation_messages = []
        
        for i, msg in enumerate(messages):
            role = msg.get("from") or msg.get("role")
            content = msg.get("value") or msg.get("content")
            
            if role in {"human", "user"}:
                conversation_messages.append({"role": "user", "content": content})
                
                # Generate assistant response
                prompt = build_conversation_prompt(conversation_messages, tokenizer, server_args)
                text = await engine.generate(prompt, sampling_params)
                
                # Update the original message data
                next_idx = i + 1
                if next_idx < len(messages):
                    next_msg = messages[next_idx]
                    if next_msg.get("from") in {"gpt", "assistant"} or next_msg.get("role") == "assistant":
                        if "value" in next_msg:
                            next_msg["value"] = text
                        else:
                            next_msg["content"] = text
                
                # Add to conversation history for next turn
                conversation_messages.append({"role": "assistant", "content": text})
        
        return data
    except Exception as e:
        print(f"Error processing conversation: {e}")
        return data
    finally:
        # Update progress bar for each completed conversation
        pbar.update(1)


async def run(
    server_args: ServerArgs,
    input_path: str,
    output_path: str,
    batch_size: int,
) -> None:
    # Load tokenizer for chat template
    print(f"Loading tokenizer from {server_args.model_path}...")
    try:
        tokenizer = AutoTokenizer.from_pretrained(
            server_args.model_path, 
            trust_remote_code=True
        )
        print(f"✓ Tokenizer loaded successfully")
        if hasattr(tokenizer, 'chat_template') and tokenizer.chat_template:
            print(f"✓ Found HF chat template in tokenizer")
        else:
            print(f"⚠ No HF chat template found, will use SGLang templates")
    except Exception as e:
        print(f"⚠ Failed to load tokenizer: {e}, will use SGLang templates only")
        tokenizer = None

    engine = InferenceEngine(**dataclasses.asdict(server_args))
    sampling_params = {"temperature": 0.8, "top_p": 0.95}

    print("Counting total conversations...")
    total_lines = 0
    with open(input_path, "r", encoding="utf-8") as fin:
        for _ in fin:
            total_lines += 1
    
    print(f"Total conversations to process: {total_lines}")
    print(f"Batch size: {batch_size}")
    print(f"Estimated batches: {(total_lines + batch_size - 1) // batch_size}")

    async def process_batch(batch: List[Dict[str, Any]], pbar: tqdm, batch_num: int) -> List[Dict[str, Any]]:
        pbar.set_postfix_str(f"Batch {batch_num}, Size: {len(batch)}")
        tasks = [
            asyncio.create_task(
                _process_conversation(item, engine, server_args, sampling_params, tokenizer, pbar)
            )
            for item in batch
        ]
        results = await asyncio.gather(*tasks)
        return results

    results: List[Dict[str, Any]] = []
    batch: List[Dict[str, Any]] = []
    batch_num = 0
    
    with tqdm(total=total_lines, desc="Processing conversations", unit="conv", 
              smoothing=0.1, dynamic_ncols=True, 
              bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}{postfix}]') as pbar:
        with open(input_path, "r", encoding="utf-8") as fin:
            for line in fin:
                batch.append(json.loads(line))
                if len(batch) >= batch_size:
                    batch_num += 1
                    batch_results = await process_batch(batch, pbar, batch_num)
                    results.extend(batch_results)
                    batch.clear()
            if batch:
                batch_num += 1
                batch_results = await process_batch(batch, pbar, batch_num)
                results.extend(batch_results)

    print(f"\nSaving results to {output_path}...")
    with open(output_path, "w", encoding="utf-8") as fout:
        for item in results:
            fout.write(json.dumps(item, ensure_ascii=False) + "\n")
    
    print(f"Processing completed! Total conversations processed: {len(results)}")
    engine.shutdown()


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--input", type=str, required=True, help="Input JSONL file")
    parser.add_argument("--output", type=str, required=True, help="Output JSONL file")
    parser.add_argument(
        "--batch-size",
        type=int,
        default=2048,
        help="Number of conversations to process concurrently",
    )
    ServerArgs.add_cli_args(parser)
    args = parser.parse_args()

    server_args = ServerArgs.from_cli_args(args)
    concurrency = args.batch_size or server_args.dp_size
    asyncio.run(run(server_args, args.input, args.output, concurrency))