import json
import os
from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import Optional, Tuple

import torch
from safetensors import safe_open
from transformers import AutoTokenizer
from transformers.cache_utils import DynamicCache

from sgl_eagle.modeling.auto_model import AutoModelForCausalLM

from .tree import initialize_tree, tree_decoding, update_inference_inputs


@dataclass
class Usage:
    """
    The runtime stats during inference when using EagleRunner.

    Args:
        num_generated_tokens: The number of tokens generated by the eagle model.
        accepted_tokens: The number of tokens generated by the draft model and accepted by the base model.
    """

    num_generated_tokens: int
    accepted_tokens: int

    def __str__(self):
        return f"""
Usage:
    num_generated_tokens: {self.num_generated_tokens}
    accepted_tokens: {self.accepted_tokens}
"""


class EagleRunner:
    """
    This class is a simple runner to run inference with Eagle3. It serves as a simple verifier of the speedup of Eagle3.
    In case you want to use more advanced features such kernel optimization and distributed inference, you should try SGLang.
    """

    def __init__(self, base_model_path, draft_model_path):
        # TODO: support loading from the sgl-ealge lib
        self.base_model = (
            AutoModelForCausalLM.from_pretrained(
                base_model_path, torch_dtype=torch.bfloat16
            )
            .cuda()
            .eval()
        )
        self.draft_model = (
            AutoModelForCausalLM.from_pretrained(
                draft_model_path, torch_dtype=torch.bfloat16
            )
            .cuda()
            .eval()
        )
        self.tokenizer = AutoTokenizer.from_pretrained(base_model_path)

    @torch.inference_mode()
    def run(
        self, prompt: str, enable_eagle3: bool = True, max_new_tokens: int = 512
    ) -> str:
        """
        Run the eagle model with the given prompt and sampling parameters.

        Args:
            prompt (str): The prompt to run the eagle model with.
            enable_eagle (bool): Whether to enable eagle.

        Returns:
            Tuple[str, Usage]: The generated text and the usage stats.
        """
        # apply chat template if any
        messages = [
            {"role": "user", "content": prompt},
        ]
        input_ids = self.tokenizer.apply_chat_template(
            messages, add_generation_prompt=True, return_tensors="pt"
        )
        input_ids = input_ids.to(self.base_model.device)

        if enable_eagle3:
            gen_func = self._eagle_generate
        else:
            gen_func = self._naive_generate

        output = []
        for output_id in gen_func(input_ids, max_new_tokens):
            output.append(output_id)
        output = torch.cat(output, dim=-1)
        output = self.tokenizer.batch_decode(output, skip_special_tokens=True)

        if enable_eagle3:
            usage = Usage(
                num_generated_tokens=output.shape[-1],
                accepted_tokens=0,
            )
        else:
            usage = Usage(num_generated_tokens=max_new_tokens, accepted_tokens=0)
        return output, usage

    def _naive_generate(self, input_ids: str, max_new_tokens: int):
        input_ids = input_ids.clone()

        # Initialize the past key and value states
        past_key_values = DynamicCache()
        input_len = input_ids.shape[1]
        outputs = self.base_model(input_ids, past_key_values=past_key_values)
        new_token = 0

        for idx in range(max_new_tokens):
            input_id = outputs.logits[:, -1:].argmax(dim=-1)

            outputs = self.base_model(input_id, past_key_values=past_key_values)
            input_ids = torch.cat([input_ids, input_id], dim=-1)
            new_token += 1

            yield input_id

            # stop when following conditions are met
            if self.tokenizer.eos_token_id in input_ids[0, input_len:].tolist():
                break
            if new_token > max_new_tokens:
                break

    def _eagle_generate(
        self,
        input_ids: str,
        max_new_tokens: int,
        num_spec_steps: int = 3,
        num_draft_tokens: int = 4,
    ) -> str:
        padding = (torch.zeros(1, 1, dtype=torch.long) - 1).to(input_ids.device)
        past_key_values = DynamicCache()
        input_len = input_ids.shape[1]

        # TODO: support logits processor
        (
            draft_tokens,
            retrieve_indices,
            tree_mask,
            tree_position_ids,
            logits,
            hidden_state,
            sample_token,
        ) = initialize_tree(
            input_ids=input_ids,
            base_model=self.base_model,
            draft_model=self.draft_model,
            past_key_values=past_key_values,
            logits_processor=None,
            num_spec_steps=num_spec_steps,
            num_draft_tokens=num_draft_tokens,
        )

        # tracking stats
        new_token = 0
        accepted_tokens = 0

        # generate response
        for idx in range(max_new_tokens):
            self.base_model.model.tree_mask = tree_mask
            draft_tokens = draft_tokens.to(input_ids.device)

            logits, hidden_state_new, outputs = tree_decoding(
                self,
                draft_tokens,
                past_key_values,
                tree_position_ids,
                input_ids,
                retrieve_indices,
            )

            draft_tokens = torch.cat((draft_tokens, padding), dim=1)
            candidates = draft_tokens[0, retrieve_indices]
            best_candidate, accept_length, sample_p = evaluate_posterior(
                logits, candidates, logits_processor
            )

            # do verification and update the output
            (
                input_ids,
                draft_tokens,
                retrieve_indices,
                tree_mask,
                tree_position_ids,
                new_token,
                hidden_state,
                sample_token,
            ) = update_inference_inputs(
                input_ids,
                candidates,
                best_candidate,
                accept_length,
                retrieve_indices,
                logits_processor,
                new_token,
                past_key_values_data,
                current_length_data,
                self,
                hidden_state_new,
                sample_p,
            )

            yield input_ids

            if self.tokenizer.eos_token_id in input_ids[0, input_len:].tolist():
                break
            if new_token > max_new_tokens:
                break


class EagleTrainer(ABC):
    def __init__(self, draft_model_path) -> None:
        self.draft_model = AutoModelForCausalLM.from_pretrained(draft_model_path)

    def __init__(self, draft_model_path):
        self.draft_model = AutoModelForCausalLM.from_pretrained(
            draft_model_path, torch_dtype=torch.bfloat16
        )

    def step(self):
        pass


class OnlineEagleTrainer(EagleRunner):

    def step(self, input_ids, attention_mask) -> torch.Tensor:
        pass


class OfflineEagleTrainer(EagleTrainer):
    def __init__(self, draft_model, base_path, tokenizer):
        self.draft_model = draft_model
        self.tokenizer = tokenizer
        self._load_embedding_and_lm_head(base_path)

        for param in self.embed_tokens.parameters():
            param.requires_grad = False

        self.draft_model.lm_head = self.draft_model.lm_head

    def _load_embedding_and_lm_head(self, base_model_path: str):
        # TODO Each model implements its own LM head and embedding logic.
        with open(
            os.path.join(base_model_path, "model.safetensors.index.json"), "r"
        ) as f:
            index_json = json.loads(f.read())
            emb_path = index_json["weight_map"]["model.embed_tokens.weight"]
            lm_head_path = index_json["weight_map"]["lm_head.weight"]
        with safe_open(
            os.path.join(base_model_path, emb_path), framework="pt", device="cpu"
        ) as f:
            tensor_slice = f.get_slice("model.embed_tokens.weight")
            vocab_size, hidden_dim = tensor_slice.get_shape()
            tensor_emb = tensor_slice[:, :hidden_dim].float()

        with safe_open(
            os.path.join(base_model_path, lm_head_path), framework="pt", device="cpu"
        ) as f:
            tensor_slice = f.get_slice("lm_head.weight")
            vocab_size, hidden_dim = tensor_slice.get_shape()
            tensor_lm_head = tensor_slice[:, :hidden_dim].float()

        self.draft_model.lm_head.weight = torch.nn.Parameter(tensor_lm_head.cuda())
        # 200018 is the padding token id for llama4
        self.embed_tokens = torch.nn.Embedding(
            vocab_size, hidden_dim, _weight=tensor_emb.cuda()
        )

    def step(
        self,
        hidden_states,
        input_ids,
        target_hidden_states,
        loss_mask,
        attention_mask: Optional[torch.Tensor] = None,
        ttt_length: int = 1,
    ) -> torch.Tensor:

        if ttt_length == 1:
            return self.step_naive(
                hidden_states=hidden_states,
                input_ids=input_ids,
                attention_mask=attention_mask,
                target_hidden_states=target_hidden_states,
                loss_mask=loss_mask,
            )
        else:
            raise NotImplementedError(
                "Only ttt_length = 1 is supported in the current version."
            )

    def step_naive(
        self,
        hidden_states: torch.Tensor,
        input_ids: torch.Tensor,
        target_hidden_states: torch.Tensor,
        loss_mask: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
    ):
        """
        Naive step function is for ttt = 1
        """
        with torch.no_grad():
            input_embeds = self.embed_tokens(input_ids)

        output = self.draft_model(
            inputs_embeds=input_embeds,
            hidden_states=hidden_states,
            attention_mask=attention_mask,
        )

        # process target logic
        with torch.no_grad():
            target_logits = self.draft_model.lm_head(target_hidden_states)
            target_logits = target_logits.float()
            target_p = torch.nn.Softmax(dim=2)(target_logits)
            target_p = target_p.detach()

        # calculate loss
        draft_logits = self.draft_model.lm_head(output)
        draft_logits = draft_logits.float()
        draft_p = torch.nn.LogSoftmax(dim=2)(draft_logits)
        plogp = target_p * draft_p
        loss = -torch.sum(loss_mask * plogp, dim=2).mean()

        return loss
